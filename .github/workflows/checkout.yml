name: Deploy DAGs to Airflow Cluster
on: push
jobs:
  checkout-airflow-worker01:
    runs-on: airflow-worker01
    steps:
      - name: checkout DAGs
        uses: actions/checkout@v3
      - name: remove dags directory
        run: rm -rf /home/ubuntu/airflow/dags
      - name: copy DAGs
        run: cp -r $(pwd)/airflow_dags /home/ubuntu/airflow/dags
  checkout-airflow-worker02:
    runs-on: airflow-worker02
    steps:
      - name: checkout DAGs
        uses: actions/checkout@v3
      - name: remove dags directory
        run: rm -rf /home/ubuntu/airflow/dags
      - name: copy DAGs
        run: cp -r $(pwd)/airflow_dags /home/ubuntu/airflow/dags
  checkout-airflow-primary:
    runs-on: airflow-primary
    steps:
      - name: checkout DAGs
        uses: actions/checkout@v3
      - name: remove dags directory
        run: rm -rf /home/ubuntu/airflow/dags
      - name: copy DAGs
        run: cp -r $(pwd)/airflow_dags /home/ubuntu/airflow/dags
  deploy-to-s3:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    steps:
      - name: pwd
        run: pwd
      - name: deploy spark scripts to s3 
        run: aws s3 sync --delete --region ap-northeast-2 spark_scripts s3://tlc-taxi/scripts/
      
